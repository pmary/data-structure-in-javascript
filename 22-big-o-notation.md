# 2.2 Big-O Notation

When trying to characterize an algorithm’s efficiency in terms of execution time, independent of any particular program or computer, it is important to quantify the number of operations or steps that the algorithm will require. If each of these steps is considered to be a basic unit of computation, then the execution time for an algorithm can be expressed as the number of steps required to solve the problem. Deciding on an appropriate basic unit of computation can be a complicated problem and will depend on how the algorithm is implemented.

A good basic unit of computation for comparing the summation algorithms shown earlier might be to count the number of assignment statements performed to compute the sum. In the function `sumOfN`, the number of assignment statements is 1 \(_theSum=0_\) plus the value of n \(the number of times we perform _theSum = theSum + i_\). We can denote this by a function, call it T, where $$T(n) = 1 + n$$. The parameter _n_ is often referred to as the "size of the problem", and we can read this as "$$T(n)$$ is the time it takes to solve a problem of size n, namely 1+n steps".

In the summation functions given above, it makes sense to use the number of terms in the summation to denote the size of the problem. We can then say that the sum of the first 100,000 integers is a bigger instance of the summation problem than the sum of the first 1,000. Because of this, it might seem reasonable that the time required to solve the larger case would be greater than for the smaller case. Our goal then is to show how the algorithm’s execution time changes with respect to the size of the problem.

Computer scientists prefer to take this analysis technique one step further. It turns out that the exact number of operations is not as important as determining the most dominant part of the $$T(n)$$ function. In other words, as the problem gets larger, some portion of the $$T(n)$$ function tends to overpower the rest. This dominant term is what, in the end, is used for comparison. The order of magnitude function describes the part of $$T(n)$$ that increases the fastest as the value of n increases. **Order of magnitude** is often called **Big-O notation** \(for "order"\) and written as $$O(f(n))$$. It provides a useful approximation to the actual number of steps in the computation. The function $$f(n)$$ provides a simple representation of the dominant part of the original $$T(n)$$.

In the above example, $$T(n)=1+n$$. As n gets large, the constant 1 will become less and less significant to the final result. If we are looking for an approximation for $$T(n)$$, then we can drop the 1 and simply say that the running time is $$O(n)$$. It is important to note that the 1 is certainly significant for $$T(n)$$. However, as n gets large, our approximation will be just as accurate without it.

As another example, suppose that for some algorithm, the exact number of steps is $$T(n)=5n2+27n+1005$$. When n is small, say 1 or 2, the constant 1005 seems to be the dominant part of the function. However, as n gets larger, the $$n^2$$ term becomes the most important. In fact, when n is really large, the other two terms become insignificant in the role that they play in determining the final result. Again, to approximate $$T(n)$$ as n gets large, we can ignore the other terms and focus on $$5^2$$. In addition, the coefficient $$5$$ becomes insignificant as n gets large. We would say then that the function $$T(n)$$ has an order of magnitude $$f(n)=n2$$, or simply that it is $$O(n2)$$.

